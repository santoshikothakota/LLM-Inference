# ğŸ§  Local LLM Inference using llama.cpp

This project demonstrates how to run Large Language Models (LLMs) locally using **llama.cpp** and **llama-cpp-python**, without relying on cloud APIs.  

The goal of this project is to understand the internal workflow of LLM inference and implement a privacy-focused AI system that runs entirely on a local machine.

---

## ğŸš€ Project Overview

Large Language Models are often accessed through cloud APIs. In this project, the model runs completely offline using quantized `.gguf` files and CPU-based inference.

This approach ensures:
- No external API dependency
- No data sharing with third-party services
- Reduced operational cost
- Greater control over model execution

---

## âœ¨ Key Features

- âœ… Local LLM inference on CPU
- âœ… Supports quantized GGUF models
- âœ… Privacy-preserving AI execution
- âœ… Lightweight Python implementation
- âœ… Simple and modular structure

---

## ğŸ›  Tech Stack

- **Python**
- **llama-cpp-python**
- **llama.cpp backend**
- **Mistral 7B (GGUF format)**

---

## ğŸ“‚ Project Structure

LLM-Inference/
â”‚
â”œâ”€â”€ run.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md


---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/santoshikothakota/LLM-Inference.git
cd LLM-Inference
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Download a GGUF Model

Download a quantized .gguf model file (e.g., Mistral 7B).

Update the model_path variable inside run.py:

model_path = "path_to_your_model.gguf"

4ï¸âƒ£ Run the Application
python run.py



